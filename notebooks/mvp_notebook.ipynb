{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Topography - MVP Notebook\n",
    "\n",
    "This notebook implements the first milestone: CSV ingestion, normalization, and deduplication.\n",
    "\n",
    "## Overview\n",
    "1. **Setup**: Import libraries and configure settings\n",
    "2. **Load Goodreads CSV**: Read and validate the CSV export\n",
    "3. **Normalize & Deduplicate**: Clean data and remove duplicates\n",
    "4. **Smoke Test**: Verify the pipeline works with sample data\n",
    "5. **Summary**: Review results and next steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standard imports\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Setup environment for Colab (must happen before other imports)\n",
    "if 'google.colab' in sys.modules:\n",
    "    import subprocess\n",
    "    from pathlib import Path\n",
    "    \n",
    "    # Check if repository is already cloned\n",
    "    repo_path = Path('/content/ReadingTopography')\n",
    "    if not repo_path.exists():\n",
    "        print('Cloning repository...')\n",
    "        subprocess.run(['git', 'clone', 'https://github.com/olivialynn/ReadingTopography.git'], \n",
    "                      cwd='/content', check=True)\n",
    "        print('\u2713 Repository cloned')\n",
    "    else:\n",
    "        print('\u2713 Repository already available')\n",
    "    \n",
    "    # Add to Python path\n",
    "    sys.path.insert(0, '/content/ReadingTopography')\n",
    "else:\n",
    "    # Local Jupyter - add parent directory to path\n",
    "    sys.path.insert(0, os.path.abspath('..'))\n",
    "\n",
    "# Now we can import from src (repo is cloned and in path)\n",
    "from src.notebook_utils import setup_notebook_environment, get_data_path\n",
    "\n",
    "# Complete environment initialization\n",
    "env = setup_notebook_environment()\n",
    "\n",
    "# Import our data ingestion functions\n",
    "from src.data_ingestion import (\n",
    "    load_goodreads_csv,\n",
    "    normalize_title,\n",
    "    normalize_author,\n",
    "    deduplicate_books,\n",
    "    process_goodreads_csv\n",
    ")\n",
    "\n",
    "# Display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 20)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print('\u2713 Setup complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Goodreads CSV\n",
    "\n",
    "Load the Goodreads export CSV and filter to \"to-read\" shelf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to your Goodreads CSV export\n",
    "# This works in both Colab and local environments\n",
    "CSV_PATH = get_data_path('sample_goodreads.csv')\n",
    "\n",
    "# Load CSV with validation\n",
    "df_raw = load_goodreads_csv(str(CSV_PATH), filter_shelf='to-read')\n",
    "\n",
    "print(f\"\\nLoaded {len(df_raw)} books from {CSV_PATH}\")\n",
    "print(f\"\\nColumns available: {', '.join(df_raw.columns)}\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Normalize & Deduplicate\n",
    "\n",
    "Apply normalization to title and author fields, then remove duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply normalization\n",
    "df_raw['title_norm'] = df_raw['Title'].apply(normalize_title)\n",
    "df_raw['author_norm'] = df_raw['Author'].apply(normalize_author)\n",
    "\n",
    "# Handle ISBN13\n",
    "if 'ISBN13' in df_raw.columns:\n",
    "    df_raw['isbn13'] = df_raw['ISBN13'].astype(str).replace('nan', '')\n",
    "elif 'ISBN' in df_raw.columns:\n",
    "    df_raw['isbn13'] = df_raw['ISBN'].astype(str).replace('nan', '')\n",
    "else:\n",
    "    df_raw['isbn13'] = ''\n",
    "\n",
    "print(\"\u2713 Normalization applied\")\n",
    "print(\"\\nExample normalizations:\")\n",
    "print(df_raw[['Title', 'title_norm', 'Author', 'author_norm']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deduplicate\n",
    "df_clean = deduplicate_books(df_raw)\n",
    "\n",
    "print(f\"\\n\u2713 Deduplication complete\")\n",
    "print(f\"Original count: {len(df_raw)}\")\n",
    "print(f\"After deduplication: {len(df_clean)}\")\n",
    "print(f\"Duplicates removed: {len(df_raw) - len(df_clean)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run complete pipeline in one step\n",
    "df_processed = process_goodreads_csv(str(CSV_PATH), filter_shelf='to-read')\n",
    "\n",
    "print(f\"\\n\u2713 Pipeline complete\")\n",
    "print(f\"\\nFinal dataset: {len(df_processed)} unique books\")\n",
    "print(f\"\\nColumns in output:\")\n",
    "print(df_processed.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run complete pipeline in one step\n",
    "df_processed = process_goodreads_csv(CSV_PATH, filter_shelf='to-read')\n",
    "\n",
    "print(f\"\\n\u2713 Pipeline complete\")\n",
    "print(f\"\\nFinal dataset: {len(df_processed)} unique books\")\n",
    "print(f\"\\nColumns in output:\")\n",
    "print(df_processed.columns.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display processed data\n",
    "print(\"Processed books:\")\n",
    "display_cols = ['Title', 'Author', 'title_norm', 'author_norm', 'isbn13']\n",
    "df_processed[display_cols].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Summary Statistics\n",
    "\n",
    "Review the data quality and coverage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"DATA QUALITY SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "total_books = len(df_processed)\n",
    "has_isbn = (df_processed['isbn13'] != '').sum()\n",
    "missing_isbn = total_books - has_isbn\n",
    "\n",
    "print(f\"\\nTotal unique books: {total_books}\")\n",
    "print(f\"\\nISBN Coverage:\")\n",
    "print(f\"  - With ISBN13: {has_isbn} ({100*has_isbn/total_books:.1f}%)\")\n",
    "print(f\"  - Without ISBN13: {missing_isbn} ({100*missing_isbn/total_books:.1f}%)\")\n",
    "\n",
    "print(f\"\\nNormalization:\")\n",
    "print(f\"  - All titles normalized: {(df_processed['title_norm'] != '').all()}\")\n",
    "print(f\"  - All authors normalized: {(df_processed['author_norm'] != '').all()}\")\n",
    "\n",
    "print(f\"\\nSample normalized identifiers:\")\n",
    "for idx in range(min(3, len(df_processed))):\n",
    "    row = df_processed.iloc[idx]\n",
    "    print(f\"  {idx+1}. '{row['Title']}' by {row['Author']}\")\n",
    "    print(f\"     \u2192 title_norm: '{row['title_norm']}'\")\n",
    "    print(f\"     \u2192 author_norm: '{row['author_norm']}'\")\n",
    "    if row['isbn13']:\n",
    "        print(f\"     \u2192 isbn13: {row['isbn13']}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Steps\n",
    "\n",
    "With the foundation in place, the next milestones are:\n",
    "\n",
    "1. **Enrichment**: Fetch metadata from Google Books and Open Library APIs\n",
    "2. **Difficulty Estimation**: Compute readability metrics from text samples\n",
    "3. **Visualization**: Create interactive Plotly scatter plot\n",
    "4. **Caching**: Store enrichment results to avoid repeated API calls\n",
    "\n",
    "See [TDD Section 15](../docs/tdd.md#15-milestones-implementation-plan) for the full implementation plan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Appendix: Testing Individual Functions\n",
    "\n",
    "You can test individual normalization functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test normalization functions\n",
    "test_cases = [\n",
    "    (\"The Great Gatsby\", \"F. Scott Fitzgerald\"),\n",
    "    (\"Harry Potter and the Sorcerer's Stone\", \"J.K. Rowling\"),\n",
    "    (\"The Lord of the Rings: The Fellowship of the Ring\", \"J.R.R. Tolkien\")\n",
    "]\n",
    "\n",
    "print(\"Normalization examples:\")\n",
    "print(\"-\" * 80)\n",
    "for title, author in test_cases:\n",
    "    print(f\"Original: '{title}' by {author}\")\n",
    "    print(f\"  \u2192 title_norm:  '{normalize_title(title)}'\")\n",
    "    print(f\"  \u2192 author_norm: '{normalize_author(author)}'\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}